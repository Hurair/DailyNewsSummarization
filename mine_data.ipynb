{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*-coding: utf-8 -*-\n",
    "import feedparser\n",
    "import string\n",
    "import sqlite3 as sqlite\n",
    "import sys\n",
    "import newspaper\n",
    "from newspaper import Article, news_pool\n",
    "import threading, time\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rssSources = {\n",
    "    'Al Jazeera':'https://www.aljazeera.com/xml/rss/all.xml',\n",
    "    'BBC News':'http://feeds.bbci.co.uk/news/world/rss.xml',\n",
    "    'The New York Times':'https://www.nytimes.com/svc/collections/v1/publish/https://www.nytimes.com/section/world/rss.xml',\n",
    "    'Global Issues':'http://www.globalissues.org/news/feed',\n",
    "    'Reuters':'http://feeds.reuters.com/Reuters/worldNews',\n",
    "    'Daily Express':'http://feeds.feedburner.com/daily-express-world-news',\n",
    "    'The Guardian':'https://www.theguardian.com/world/rss',\n",
    "    'The Independent': 'http://www.independent.co.uk/news/world/rss',\n",
    "    'The Sun Daily': 'http://www.thesundaily.my/rss/world',\n",
    "    'CNBC': 'https://www.cnbc.com/id/100727362/device/rss/rss.html'\n",
    "}\n",
    "twitCred = {\n",
    "    'consumer_key':'1OgJMHusv7DjyvkqCJADEt1Xn',\n",
    "    'consumer_secret':'rfj2mHi8qKS1gQtrl2QBoGmXK6jcTi0ann5R3Kn877HCQBNf5n',\n",
    "    'access_token':'1134350418-s6h7wX2M9GGsM4Me67RIcmt4ZjBeYrl3R1heiFe',\n",
    "    'access_token_secret':'elN0VJ8vo8Oioh5pTZeGcu5MHFCNzudXB4ktTM5cSh4kD'\n",
    "}\n",
    "twitSources = {\n",
    "    'CBS News':'@CBSNews',\n",
    "    'Fox News': '@FoxNews',\n",
    "    'BuzzFeed News': '@BuzzFeedNews',\n",
    "    'The Associated Press':'@AP'\n",
    "}\n",
    "webSources = {\n",
    "    'CNN':'https://edition.cnn.com/specials/last-50-stories',\n",
    "    'Global News': 'https://globalnews.ca/world/',\n",
    "    'AU News': 'https://www.news.com.au/world/breaking-news'\n",
    "}\n",
    "instaSources = {\n",
    "    'TODAY': 'https://www.instagram.com/todayonline/',\n",
    "    'The Economist': 'https://www.instagram.com/theeconomist/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class setInterval :\n",
    "    def __init__(self,interval,action) :\n",
    "        self.interval=interval\n",
    "        self.action=action\n",
    "        self.stopEvent=threading.Event()\n",
    "        thread=threading.Thread(target=self.__setInterval)\n",
    "        thread.start()\n",
    "\n",
    "    def __setInterval(self) :\n",
    "        nextTime=time.time()+self.interval\n",
    "        while not self.stopEvent.wait(nextTime-time.time()) :\n",
    "            nextTime+=self.interval\n",
    "            self.action()\n",
    "\n",
    "    def cancel(self) :\n",
    "        self.stopEvent.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRSSObj(x, value):\n",
    "    tags = time = author = ''\n",
    "    if hasattr(x, 'tags'):\n",
    "        for tag in x.tags:\n",
    "            if(tags == ''):\n",
    "                tags = tag.term\n",
    "            else:\n",
    "                tags += '|' + tag.term\n",
    "                \n",
    "    if hasattr(x, 'published_parsed'):\n",
    "        time = x.published_parsed\n",
    "    else:\n",
    "        time = x.updated_parsed\n",
    "        \n",
    "    if hasattr(x, 'author'):\n",
    "        author = x.author\n",
    "        \n",
    "    time = datetime.fromtimestamp(mktime(time))\n",
    "\n",
    "    return (x.title,x.link,x.summary,time,author,tags,value)\n",
    "\n",
    "def getTweetObj(x, key, value):\n",
    "    urls = tags = ''\n",
    "    for url in x.entities['urls']:\n",
    "        if(urls == ''):\n",
    "            urls = url['url']\n",
    "        else:\n",
    "            urls += '|' + url['url']\n",
    "        \n",
    "    for tag in x.entities['hashtags']:\n",
    "        if(tags == ''):\n",
    "            tags = tag['text']\n",
    "        else:\n",
    "            tags += '|' + tag['text']\n",
    "        \n",
    "    return (x.id,x.full_text,urls,x.created_at,tags,value,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDB():\n",
    "    with sqlite.connect(r'news.db') as con: \n",
    "        cur = con.cursor()\n",
    "        \n",
    "        cur.execute(\"drop table if exists rss;\")\n",
    "        cur.execute(\"create table rss(id integer primary key autoincrement, title text unique, link text unique, description text, published_at datetime, authors text, tags text, source text)\")\n",
    "    \n",
    "        cur.execute(\"drop table if exists web;\")\n",
    "        cur.execute(\"create table web(id integer primary key autoincrement, title text unique, link text unique, description text, published_at datetime, authors text, tags text, source text)\")\n",
    "        \n",
    "        cur.execute(\"drop table if exists twitter;\")\n",
    "        cur.execute(\"create table twitter(id integer primary key, tweet text, link text, published_at datetime, hashtags text, sourceHandle text, source text)\")\n",
    "        \n",
    "        cur.execute(\"drop table if exists instagram;\")\n",
    "        cur.execute(\"create table instagram(id integer primary key, caption text, link text, published_at datetime, sourceHandle text, source text)\")\n",
    "        \n",
    "        con.commit()\n",
    "\n",
    "#cleanDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses the RSS feed from RSS\n",
    "def parseRSS():\n",
    "    rssNews = []\n",
    "    for key, value in rssSources.items():\n",
    "        d = feedparser.parse(value)\n",
    "        rssNews += [getRSSObj(x, key) for x in d['items']]\n",
    "\n",
    "    with sqlite.connect(r'news.db') as con: \n",
    "        cur = con.cursor()\n",
    "        cur.executemany(\"insert or ignore into rss(title,link,description,published_at,authors,tags,source) values(?,?,?,?,?,?,?)\", rssNews)\n",
    "        con.commit()\n",
    "        \n",
    "#parseRSS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run after every 5 min interval\n",
    "interRss=setInterval(1800,parseRSS)\n",
    "\n",
    "# will stop interval in 1hr\n",
    "trss=threading.Timer(21600,interRss.cancel)\n",
    "trss.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "since_id = {}\n",
    "for key, value in twitSources.items():\n",
    "    since_id[key] = None\n",
    "    \n",
    "def getTweets():\n",
    "    twitNews = []\n",
    "    auth = tweepy.OAuthHandler(twitCred['consumer_key'], twitCred['consumer_secret'])\n",
    "    auth.set_access_token(twitCred['access_token'], twitCred['access_token_secret'])\n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "    \n",
    "    for key, value in twitSources.items():\n",
    "        allTweets = tweepy.Cursor(\n",
    "            api.user_timeline,\n",
    "            id=value,\n",
    "            screen_name=value,\n",
    "            trim_user=True,\n",
    "            include_rts=False,\n",
    "            tweet_mode='extended',\n",
    "            since_id = since_id[key]\n",
    "            ).items(100)\n",
    "        \n",
    "        indTwit = [getTweetObj(x, key, value) for x in allTweets]\n",
    "        if(len(indTwit)):\n",
    "            since_id[key] = indTwit[0][0]\n",
    "            twitNews += indTwit\n",
    "\n",
    "    with sqlite.connect(r'news.db') as con: \n",
    "        cur = con.cursor()\n",
    "        cur.executemany(\"insert or ignore into twitter values(?,?,?,?,?,?,?)\", twitNews)\n",
    "        con.commit()\n",
    "        \n",
    "#getTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run after every 5 min interval\n",
    "interTwit=setInterval(1800,getTweets)\n",
    "\n",
    "# will stop interval in 1hr\n",
    "ttwit=threading.Timer(21600,interTwit.cancel)\n",
    "ttwit.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses the WebPages from News Websites\n",
    "def scrapWebsites():\n",
    "    webNews = []\n",
    "    for key, value in webSources.items():\n",
    "        paper = newspaper.build(value, memoize_articles=True, fetch_images=False)\n",
    "\n",
    "        count = 1\n",
    "        for content in paper.articles:\n",
    "            try:\n",
    "                content.download()\n",
    "                content.parse()\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                continue\n",
    "\n",
    "            if content.publish_date is None:\n",
    "                continue\n",
    "                \n",
    "            authors = tags = ''\n",
    "            for author in content.authors:\n",
    "                if(authors == ''):\n",
    "                    authors = author\n",
    "                else:\n",
    "                    authors += '|' + author\n",
    "                    \n",
    "            for tag in content.tags:\n",
    "                if(tags == ''):\n",
    "                    tags = tag\n",
    "                else:\n",
    "                    tags += '|' + tag\n",
    "            webNews.append((content.title,content.url,content.text,content.publish_date,authors,tags,key))\n",
    "\n",
    "            #print(count, \"articles downloaded from\", key, \"using newspaper, url: \", content.url)\n",
    "            count = count + 1\n",
    "\n",
    "            if(count>20):\n",
    "                break\n",
    "        \n",
    "    with sqlite.connect(r'news.db') as con: \n",
    "        cur = con.cursor()\n",
    "        cur.executemany(\"insert or ignore into web(title,link,description,published_at,authors,tags,source) values(?,?,?,?,?,?,?)\", webNews)\n",
    "        con.commit()\n",
    "        \n",
    "#scrapWebsites()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run after every 5 min interval\n",
    "interWeb=setInterval(1800,scrapWebsites)\n",
    "\n",
    "# will stop interval in 1hr\n",
    "tweb=threading.Timer(21600,interWeb.cancel)\n",
    "tweb.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instagram\n",
    "from instagram import InstagramScraper\n",
    "\n",
    "def getInstaPosts():\n",
    "    instaPosts = []\n",
    "    IS = InstagramScraper()\n",
    "    for key, value in instaSources.items():\n",
    "        results = IS.profile_page_recent_posts(value)\n",
    "        for result in results:\n",
    "            instaId = int(result['id'])\n",
    "            instaURL = result['display_url']\n",
    "            instaDate = datetime.fromtimestamp(result['taken_at_timestamp'])\n",
    "            instaHandle = result['owner']['username']\n",
    "            instaCaption = result['edge_media_to_caption']['edges'][0]['node']['text']\n",
    "            instaPosts.append((instaId, instaCaption, instaURL, instaDate, instaHandle, key))\n",
    "      \n",
    "    with sqlite.connect(r'news.db') as con: \n",
    "        cur = con.cursor()\n",
    "        cur.executemany(\"insert or ignore into instagram values(?,?,?,?,?,?)\", instaPosts)\n",
    "        con.commit()\n",
    "        \n",
    "#getInstaPosts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run after every 5 min interval\n",
    "interInsta=setInterval(1800,getInstaPosts)\n",
    "\n",
    "# will stop interval in 1hr\n",
    "tinsta=threading.Timer(21600,interInsta.cancel)\n",
    "tinsta.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
