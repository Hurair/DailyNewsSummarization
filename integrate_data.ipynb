{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sqlite\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAllNews():\n",
    "    with sqlite.connect(r'news.db') as con: \n",
    "        cur = con.cursor()\n",
    "        \n",
    "        cur.execute(\"drop table if exists allNews;\")\n",
    "        cur.execute(\"create table allNews(id integer primary key autoincrement, title text, link text unique, description text, published_at datetime, authors text, tags text, source text, orig_cluster integer)\")\n",
    "        \n",
    "        con.commit()\n",
    "        \n",
    "#createAllNews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regular expressions\n",
    "reg1 = re.compile(r'\\n') #Regex to replace <br /> with \\n (see reg1.sub)\n",
    "reg2 = re.compile(r'(<!--.*?-->|<[^>]*>)') #Regex to clean all html tags (anything with <something>)\n",
    "reg3 = re.compile(r'&nbsp;') #Regex to clean all &nbsp \n",
    "reg4 = re.compile(r'&apos;') #Regex to clean all ' chars\n",
    "reg5 = re.compile(r'https?:\\/\\/.*[\\r\\n]*') #Regex to remove all links\n",
    "\n",
    "def cleanText(txt):\n",
    "    first_filter = reg1.sub(' ', txt)\n",
    "    second_filter = reg2.sub(' ', first_filter)\n",
    "    third_filter = reg3.sub(' ', second_filter)\n",
    "    fourth_filter = reg4.sub('', third_filter)\n",
    "    return reg5.sub('', fourth_filter)\n",
    "\n",
    "def integrateAllNews():\n",
    "    allFeeds = []\n",
    "    prev_date = \"datetime('now','-1 days')\"\n",
    "    with sqlite.connect(r'news.db') as con: \n",
    "        cur = con.cursor()\n",
    "\n",
    "        cur.execute(\"select title, link, description, published_at, authors, tags, source from rss where published_at > \" + prev_date)\n",
    "        rows = cur.fetchall()\n",
    "        allFeeds += [(cleanText(x[0]), x[1], cleanText(x[2]), x[3], x[4], x[5], x[6]) for x in rows]\n",
    "\n",
    "        cur.execute(\"select tweet, link, published_at, hashtags, source from twitter where published_at > \" + prev_date)\n",
    "        rows = cur.fetchall()\n",
    "        allFeeds += [('', x[1], cleanText(x[0]), x[2], '', x[3], x[4]) for x in rows]\n",
    "\n",
    "        cur.execute(\"select title, link, description, published_at, authors, tags, source from web where published_at > \" + prev_date)\n",
    "        rows = cur.fetchall()\n",
    "        allFeeds += [(cleanText(x[0]), x[1], cleanText(x[2]), x[3], x[4], x[5], x[6]) for x in rows]\n",
    "\n",
    "        cur.execute(\"select caption, link, published_at, source from instagram where published_at > \" + prev_date)\n",
    "        rows = cur.fetchall()\n",
    "        allFeeds += [('', x[1], cleanText(x[0]), x[2], '', '', x[3]) for x in rows]\n",
    "        \n",
    "        cur.executemany(\"insert or ignore into allNews(title,link,description,published_at,authors,tags,source) values(?,?,?,?,?,?,?)\", allFeeds)\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrateAllNews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
